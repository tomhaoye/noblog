---
title: 极验滑动验证码行为模拟
date: 2018-09-01 21:54:55
tags: other
categories: 有趣
---

>为了能看到具体的效果，使用了`selenium`+`chrome`进行实验，如果大家想要打包下载即用，可以自行修改搭配`PhantomJS`食用。
经过一番折腾成功率已经比较高了（70%-90%吧【2018/10/01】），某些背景干扰元素过多缺口又刚好在色差较小的地方的时候可能会计算错误，也可能还有某些特殊的情况，GIF是旧的，具体可看MP4。 代码仅供参考学习交流，有兴趣可以拿去玩玩，鉴于本人能力有限，代码、逻辑以及解决方式上有错误在所难免，请各位大佬多多指教~ 
另外我发现极验官网快改版了，鉴于该代码直接在极验官网上模拟，可能很快就用不了了，届时可以考虑接入api到自己本地再进行模拟测试。

仓库地址: <a href='https://github.com/tomhaoye/geetest-crack-demo'>https://github.com/tomhaoye/geetest-crack-demo</a>


### 故事起源

之前写了个爬虫抓取58的小区数据的代码，有人提了个issue，说小区详情数据和经纬度为空，我自己试了一把，发现请求太频繁会被强制跳转到验证码的页面去。

后来试用了代理，发现一个ip没抓几条数据就被强制跳转人机校验了，然后我手动去访问网页并通过验证码的校验之后再去抓取数据，发现抓取了几千条数据也没有再被强跳了。

这是什么规则=。=算了，还是先想办法能自动通过人机校验先。而58的这个页面的人机校验，是类似于极验的那种滑动验证码。emmmm，网上资料都很多，但是版本较旧，现在的极验滑动校验改进了不少地方，不过思路总是可以参考了。

### 步骤分解

 - 屏幕出现滑动行为验证码框
 - 截取验证码合成图片\获取验证码原图片
 - 图片处理以及缺口定位
 - 将拼图移动到缺口处
 - 后续：关于干扰块排除的想法
 - 后续：关于增加定位模拟难度的想法

### 开始研究

#### 0、屏幕出现滑动行为验证码框

我首先找到使用极验控件的地方，例如：<a href="http://www.geetest.com/type/">极验官网</a>，然后点击到滑动行为验证框出现。这个行为使用`selenium`模拟比较简单，关于如何使用`selenium`进行元素定位、模拟点击等这里就不多介绍了，有需要学习的找谷歌百度皆可，有前端基础的一小时内就能够上手。

#### 1、截取验证码合成图片\获取验证码原图片

##### 1.1、获取验证码原图片

现在滑动行为验证框已经出现了，接下来应该是想办法拿到原图片，或者是直接截取当前合成好的图片，两种做法有本质上什么区别呢？首先先说一下获得原图片，极验在加载校验控件的时候会请求三张图片，其中一张是缺口原图，另外一张是完整原图，还有一张是拼块。
<center>
<img src="https://raw.githubusercontent.com/tomhaoye/geetest-crack-demo/master/src/1.jpg" alt="缺口原图">
缺口原图

<img src="https://raw.githubusercontent.com/tomhaoye/geetest-crack-demo/master/src/2.jpg" alt="完整原图">
完整原图
</center>

大家可以看到，请求的所获取到的原图，都是打散重组过的图片，我读过一些解析的文章，有的提到重新组合到一起的方法写在代码里面，而有的文章则是表示散块映射关系是有接口请求的，这也许是大家研究的版本不一样所导致的。我下载了一些原图的组合进行了观察，倒是发现了每一组原图的其实打散前后的散块的位置应该是固定的，也就是说映射关系应该是固定的。其实我觉得，无论映射关系是在前端代码里，还是在接口里，只要你需要在客户端进行重组的，那就相当于没有秘密，所以多一事不如少一事，直接使用固定的映射关系倒是省事不少。

##### 1.2、截取验证码合成图片

这一小节是关于获取图片的，图片处理会在下一小节，所以接下来说一下直接截取合成的图片。所谓所见即所得，能够获取到人眼直接看到的图像，才是最接近人类行为的模拟，而如果通过其他方式取巧，哪天他的方式改了，你的逻辑就不适用了。就像我后来观察58的滑动行为验证，它家请求的图片就是完整的合成图，所以遵循正常人类行为才是究极奥义啊。至于如何截取动行为验证框中的合成图片，我们需要定位到验证框所属的位置，于是乎我先定位到一个`class`为`geetest_window`的`div`，拿到它当前的位置以及长宽属性，接着截图整个当前页面的，并根据刚刚获得的位置以及长宽属性对当前页面图片进行裁剪，就可以获得我们人眼所看到的合成图片了。
 ```python
 def cut_gt_window_image(browser):
    image_div = browser.find_element_by_class_name("geetest_window")
    location = image_div.location
    size = image_div.size
    top, bottom, left, right = location['y'], location['y'] + size['height'], location['x'], location['x'] + size[
        'width']
    screen_shot = browser.get_screenshot_as_png()
    screen_shot = image.open(BytesIO(screen_shot))
    captcha = screen_shot.crop((left, top, right, bottom))
    captcha.save(cut_image_path)
    return browser
 ```
<center>
 <img src="https://raw.githubusercontent.com/tomhaoye/geetest-crack-demo/master/pic/cut.png" alt="合成图片">
 合成图片
</center>

通过这一小节我们对于获取图片的方式和方法已经了然于胸，现在就让我们进入下一小节，对刚刚所获得的图片进行分析和研究吧。

 #### 2、图片处理以及缺口定位

 ##### 2.1、处理打散的原图片与定位缺口位置

 在上一小节，我们已经猜测过是打散过的图片跟实际上正常的图片的映射关系是固定的，所以我先尝试对其中一张缺口原图的打散图片进行重组。重组的关键其实就是找到散块跟原来位置的映射关系就行了，于是我拿着打散图片跟合成好的图片一块一块的对比，发现了它实际上是将正常的图片分为上下两部分，然后上下两部分再分为26份进行打散的。关于位置的映射关系，由于时间关系，我就动用了我的火眼金睛直接得到了结果：
 `{1: 18, 2: 17, 3: 15, 4: 16, 5: 22, 6: 21, 7: 14, 8: 13, 9: 10, 10: 9, 11: 19, 12: 20, 13: 2, 14: 1, 15: 6, 16: 5, 17: 26, 18: 25, 19: 23, 20: 24, 21: 7, 22: 8, 23: 3, 24: 4, 25: 11, 26: 12}`
 
 这里的映射关系是图片上半部分的，而下半部分的映射关系，我找了前两个散块的原本对应位置，就大概能猜到其实就是使用上半部分的映射关系相邻两值交换后的结果，例如上半部分是`1:18, 2:17`那么到了下半部分就是`1:17, 2:18`，下面就是具体的还原图片代码：
 ```python
 def merge_img(img_path='', target=''):
    im = image.open(img_path)
    to_image = image.new('RGB', (260, 160))
    dx = 12
    dy = 80
    x = 0
    img_map = {1: 18, 2: 17, 3: 15, 4: 16, 5: 22, 6: 21, 7: 14, 8: 13, 9: 10, 10: 9, 11: 19, 12: 20, 13: 2, 14: 1,
               15: 6, 16: 5, 17: 26, 18: 25, 19: 23, 20: 24, 21: 7, 22: 8, 23: 3, 24: 4, 25: 11, 26: 12}
    while x <= 300:
        y = 0
        while y <= 80:
            from_img = im.crop((x, y, x + dx, y + dy))
            second_line = img_map[(x / 12) if ((x / 12) % 2) else (x / 12 + 2)] - 1
            loc = ((img_map[x / 12 + 1] - 1) * 10 if y else second_line * 10, abs(y - dy))
            to_image.paste(from_img, loc)
            y += dy
        x += dx
    to_image = to_image.convert('L')
    to_image.save(target)
    return to_image
 ```
补充一点，大家看到的打散图片的宽度是312个像素的，而我这里最后还原的得到的图片宽度却是260像素，这是因为打散图片的每一个相邻散块之间实际上是有重叠部分的，一开始我合成得到的宽度312像素的图片时候发现图片是变了样的，所以最后将他们重叠部分堆叠在一起，才得到了正常的原始图片。

<center>
<img src="https://raw.githubusercontent.com/tomhaoye/geetest-crack-demo/master/src/fmerged.jpg" alt="还原完整图片">
还原完整图片

<img src="https://raw.githubusercontent.com/tomhaoye/geetest-crack-demo/master/src/merged.jpg" alt="还原缺口图片">
还原缺口图片
</center>

缺口图和完整图都可以如法炮制，最后为了方便对比，进行了灰度化处理。接下来由于自身对各种图片格式并不熟悉，碰了不少壁，我会另外再写一篇关于图片格式的学习笔记，完工后会把链接贴过来，有需要的童鞋可以自取，这里我就不详细说自己怎么坑自己的了。

那我们来进入正题，两张完整的图片已经到手了，接下来就应该是定位缺口的起始位置了。童鞋们可以用肉眼看到，两张图片除了缺口位置，其他部分基本是完全一样的，所以思路很简单，就是对两张图片逐个像素点进行对比，然后第一个有差别的位置，是不是就是缺口的起始位置呢？

换作先前的版本或许是对的，但现在来说大概是错的，为什么这么说呢？比较多前辈的文章中提到第一个差异的位置就是缺口的起点，但是那是基于打散图片后使用的压缩标准是无损压缩，如果使用了有损压缩（或者说支持有损压缩的标准），那么得到的缺口图片和完整图片就不一定只有缺口部分有差异了。不过好在我们知道，即使是有损压缩，也只是损失部分细节，肉眼上看到的差异并不明显，对于计算机来说，就是颜色变化并不大，对于灰度图片来说，我们可以认为是灰度值相差较小。而我们缺口位置，因为需要通过人眼能够清晰的辨别，所以它跟周围的灰度值相差应该是比较大的。于是我们就能够想到去找到这个灰度值相差的阈值，去进行区分到底是缺口还是因为有损压缩所带来的细节损失。分析大概就是到这了，下面是具体的代码：
```python
def enlarge_diff_image(bg_path='', fbg_path='', save_path=''):
    bg_img = image.open(bg_path)
    fbg_img = image.open(fbg_path)
    img = image.new('L', (260, 160))
    for i in range(260):
        for j in range(160):
            if abs(bg_img.getpixel((i, j)) - fbg_img.getpixel((i, j))) > 40:
                img.putpixel((i, j), bg_img.getpixel((i, j)))
            else:
                img.putpixel((i, j), 255)
    img.save(save_path)
```

<center>
<img src="https://raw.githubusercontent.com/tomhaoye/geetest-crack-demo/master/src/ldiff.jpg" alt="差异瞄点图">
差异瞄点图
</center>

上面的图片就是自己坑自己的代表作，因为我将打散图还原后使用了有损压缩，最后对比的结果出来了很多干扰点，虽然通过一定的规则去定位缺口的实际位置没有太大问题，但是大家还是尽量使用无损压缩的图片进行对比会比较妥当。当然你们可以不将还原后的图片保存就直接进行对比，我这里为了方便步骤分解，躺了一次坑，学到新姿势，倒不是坏事。

<center>
<img src="https://raw.githubusercontent.com/tomhaoye/geetest-crack-demo/master/src/ldiff.bmp" alt="无损差异瞄点图">
无损差异瞄点图
</center>

得到这样的图片，大家想要求缺口起始位置与左边框的像素就十分简单了，当然我们还可以继续处理，将图片真正的二值化，不过最后求移动距离都是只要求第一个0点（黑点）的像素点就行，以防万一还可以加上对范围内0点（黑点）面积阈值判断。

<center>
<img src="https://raw.githubusercontent.com/tomhaoye/geetest-crack-demo/master/src/bin.bmp" alt="二值化图">
二值化图
</center>

缺口图和完整图的缺口定位并不太难，大概到这里就可以结束了，然我们进入下一小小节。

 ##### 2.2、处理截取的合成图片与定位缺口位置

这部分的图片的处理和分析对我来说应该是这个小demo里面最难的点了，我知道现在的人工智能领域已经比较强大了，对很多图片内容的认知甚至超过了人类，解决这个问题或许很简单。而我则是没有深入学习过这些方面的知识，但依然想凭借自己现有的技能和思想去解决这个问题。因为前面也说过，这种方式的分析才是最接近人类行为的，只要人类认知图片内容的方式不发生变化，这里面的逻辑就还能用，于是就风风火火的开干吧。
说干就干也不能立刻就写代码，毕竟我们还不知道要怎么对一张图片进行处理和分析，才能够找到那个我们需要的结果。还是先想几条路子出来吧：
 - 缺口和周围有明显的颜色区别，我们能不能以此来确定接口位置？
 - 滑块和缺口都是比较规矩的拼图形状，而且y轴的范围是一样的，我们能不能在水平方向上找到两条或多条相似的竖线？

基于上面的想法，我也是尝试做了两套方案出来。

 ###### 方案一：反正就是想办法把缺口涂黑

好饿好困啊，明天继续写。
好了今天搬完砖回家继续码字。这一方案的最初的想法就是直接根据缺口的颜色范围来确认缺口到底在图中哪个地方，但是后面实现起来却发现是有不少问题的，到最后这个方案所得出来的位置虽有些时候确实能成功，但是准确率比较低。具体问题如下：
 - 大家看到缺口处的颜色大概是由背景加上一层有透明度的灰色渐变图层所组成的，而组合出来的颜色很大程度取决于背景颜色，所以颜色范围就太大了，很容易把背景其他内容也选出来。
 - 有那么两张图背景是偏暗的，如果说缺口没有明显边界的话，那么缺口其实通过肉眼也比较难观察出来，那么这种情况下我们最终描绘出来的图片可能是一大片都是缺口区域，因为缺口和背景融为一体了。

下面是这个方案的代码，没有太多次的去修改，因为颜色范围确实是不可控的，后来干脆也灰度化再对一定灰度范围描绘了。
```python
def get_bin_image(img_path='', save_path='', t_h=150, t_l=60):
    img = image.open(img_path)
    img = img.convert('L')
    table = []
    for i in range(256):
        if i in range(t_l, t_h):
            table.append(0)
        else:
            table.append(1)
    binary = img.point(table, '1')
    binary.save(save_path)
```

<center>
<img src="https://raw.githubusercontent.com/tomhaoye/geetest-crack-demo/master/pic/bin_bg.bmp" alt="二值化结果图">
二值化结果图
</center>

emmmm，得到这个图片，其实是比较理想的情况，缺口周围没有太多的干扰元素，定位缺口的还是比较容易的。大家认真数过可以知道，缺口和拼图的宽度（不算凹凸部分）是42个像素，而拼图与边框的左边距是6像素（拼图突出部分不在左边的情况下），那么缺口的x轴范围就应该是从第49个像素开始。那么我们在这范围内进行缺口起点的查找，可以沿用之前的想法，就是第一个满足一定规律(连续的黑点数量或者比例、范围内黑点面积占比等等)的黑点，我们可以认为他比较大概率是缺口的起点。具体的代码：
```python
def get_x_point(bin_img_path=''):
    tmp_x_cur = 0
    img = image.open(bin_img_path).load()
    # 缺口出现范围大概在x轴[49-52]-220,y轴15-145
    for y_cur in range(15, 145):
        b_acc = 0
        tmp_x_cur = 0
        for x_cur in range(49, 220):
            if img[x_cur, y_cur] == 0:
                if b_acc == 0:
                    tmp_x_cur = x_cur
                b_acc += 1
            else:
                if b_acc in range(36, 44):
                    return tmp_x_cur - 40 + b_acc
                else:
                    b_acc = 0
    return tmp_x_cur
```

这个函数我后来没有继续去优化了，因为觉得想到了方案二的一些雏形，就开始着手方案二了。这个函数定义的规则相当的简陋，即使排除了上面提到的问题，有的时候都还是不能准确判断起始位置。大家有兴趣的话可以自己定制一些规则，尝试提升这种方案的成功率。

 ###### 方案二：对图片所有内容进行描边

关于这个方案，其实一开始并没有觉得比第一个方案好到哪里去，最后试验得出的结果却较为满意，我认为应该还是归功于较为完善的规则。无论是哪个方案，只要是模仿人类认知行为的，都有一定的可行性，尽管方式上有区别，但是这就跟人类认知事物的过程是一样的。例如我们在认识鸡这种动物的时候，既记住了它们的棕黄色的毛，红色的冠，也记住了它们的外形，那我们在下次看到一只乌鸡的时候，我们能够通过它的外形确认这是一只鸡，只是一只颜色不一样的鸡而已，没人会因为它的毛是黑色的而觉得它是一只黑猫。通过捕捉人类区分事物特征点的方式去思考以及编码，才是解决这次实验的关键。

好了，又到了新的一天，今天我继续说说方案二的实验过程。在这节一开始我就简单的描述过方案二，不过可能表述得并不清晰，这里再简洁的说一下，其实第一步，就是先对图像内有明显边界的东西进行描边。例子如下：

<center>
<img src="https://raw.githubusercontent.com/tomhaoye/geetest-crack-demo/master/pic/cut.png" alt="截取的原图">
截取的原图
</center>

<center>
<img src="https://raw.githubusercontent.com/tomhaoye/geetest-crack-demo/master/pic/contour.bmp" alt="描边图">
描边图
</center>

这里我们得到的图片跟原图里面的事物轮廓基本相符的，相信大家都能发现他们的相似之处，而不同之处大概表现在了内容的颜色上面。得到的黑白图片对于我们很有帮助，因为在这里它所表达的信息对于我们来说比五彩斑斓的原图直观得多。下面是代码，具体的方法其实就是通过边界与相邻像素点的颜色差异比较大而得出来的。
```python
def get_contour_image(img_path='', save_path=''):
    contour_img = image.new('1', (260, 160))
    img = image.open(img_path)
    img = img.convert('L')
    h_last_point = None
    v_last_point = None
    for x in range(260):
        for y in range(160):
            if v_last_point is not None and abs(img.getpixel((x, y)) - v_last_point) > 25:
                contour_img.putpixel((x, y), 1)
            v_last_point = img.getpixel((x, y))
    for y in range(160):
        for x in range(260):
            if h_last_point is not None and abs(img.getpixel((x, y)) - h_last_point) > 25:
                contour_img.putpixel((x, y), 1)
            h_last_point = img.getpixel((x, y))
    contour_img.save(save_path)
```

既然边界已经描绘出来了，接下来我们就是想办法去定位缺口了。方案二所得到的图片内容比较丰富，基本与原图是没有差别的，那我们就不能够通过方案一中最初规则去定位缺口了，因为在里面满足规则的色块很多很多，所以我们应该为方案二量身定做一套新规则。

现在我们需要做的是观察描边图并找到并找到隐藏在其中的一些信息，大家可以看到：缺口附近的描边十分规整，而拼块附近的描边比较粗大；缺口描边内的黑块面积应该跟拼块描边内的黑块面积差不多；缺口跟拼块在y轴上的范围基本是一样的。接下来我们就利用刚刚观察到的这些信息进行分析，看看能不能得出一定的规律，制定出适当的规则。

从上面的信息中，有的可能自身就能够成为规则，而有的则需要进行组合，也许大家能够找到更多的信息，也能制定很多的规则。而我想到的规则，也许存在不完善或者错误的的地方，但我也想在这里跟大家分享，希望大家指点指点。之前我们寻找缺口基本都是整幅图去遍历所有像素点的，这样很容易被其他的元素所干扰。所以我就想能不能先确定一个方向上（例如y轴）的范围，然后就只需要在（例如x轴上）找到满足的点或线，记录起来最后再做筛选。这里我选取了拼块的左白边作为定位y轴范围的依据，因为他开始于x轴上的第7个像素（原图），而x轴上的7之前的像素基本是没有其他元素干扰的，这让定位y轴范围变得比较方便和准确。一般来说，我们只需要找到x轴上第七个像素（索引为6）在y轴上每连续42个像素（拼块长宽度）中哪个范围白点（描边）最多，那么这个范围就是拼块和缺口左描边的范围。当然，事实上还有特殊的情况，先上个代码再来慢慢解释。
```python
def get_start_point(bin_img_path=''):
    img = image.open(bin_img_path)
    # 滑块左边位置7px[6\13]处，获取滑块位置
    _pixel = 42
    _color_diff_list = {}
    initial_slider_left_x_index_range = range(6, 14)
    for initial_slider_left_x_index in initial_slider_left_x_index_range:
        back_color_n = 0
        slider_left = {}
        for y_cur in range(118):
            color_n = 0
            for add_to_next in range(_pixel):
                color_n += img.getpixel((initial_slider_left_x_index, y_cur + add_to_next))
            slider_left[color_n] = y_cur
        w_color_n_max = max(slider_left)
        y_start_cur = slider_left[w_color_n_max]
        print(f'索引{initial_slider_left_x_index}左白值总和:{w_color_n_max}')
        for add_to_next in range(_pixel):
            back_color_n += img.getpixel((initial_slider_left_x_index + 1, y_start_cur + add_to_next))
        print(f'索引{initial_slider_left_x_index}右白值总和:{back_color_n}')
        _color_diff_list[w_color_n_max - back_color_n] = initial_slider_left_x_index
    best_point = _color_diff_list[max(_color_diff_list)]
    print(f'最佳起点:{best_point}')
    return best_point
```

大家可以看到代码中的注释：`# 滑块左边位置7px[6\13]处，获取滑块位置`，为什么不是刚刚说的第七个像素，而是第七个像素到第十四个像素这样一个范围？因为拼块它有凹凸的部分，而且凹凸的程度还有不同的情况，我相信这也是极验增加机器模拟的难度的一种手段，规矩的图形太容易被定位了。所以这里需要准确的找到拼块的左描边的位置，还需要结合描边右侧的黑点数量进行判断定位。

我们得到了真正的拼块左边描x轴位置，其实也已经得到了拼块和缺口左边描y轴的上的范围。接下来我们只要使用这个y轴范围在x轴上寻找跟拼块左描边白点数基本相等的x值就能够得出缺口起始位置x值，可能会有缺口结束位置x值，也许还有背景描边满足该条件的x值。
```python
def get_x_point_in_contour(bin_img_path=''):
    img = image.open(bin_img_path)
    # 拼块外部阴影范围
    _shadow_width = 5
    _pixel = 42
    # 滑块左边位置7px[6\13]处（考虑凸在左的情况），获取滑块位置
    slider_left_x_index = get_start_point(bin_img_path)
    slider_left = {}
    for y_cur in range(118):
        color_n = 0
        for add_to_next in range(_pixel):
            color_n += img.getpixel((slider_left_x_index, y_cur + add_to_next))
        slider_left[color_n] = y_cur
    y_max_col = max(slider_left)
    print(f'滑块左边白值总和:{y_max_col}')
    y_start_cur = slider_left[y_max_col]
    print(f'缺口图像y轴初始位置:{y_start_cur}')
    # 缺口出现范围大概在x轴[48-52+拼块阴影]-220
    gap_left = {}
    for x_cur in range(slider_left_x_index + _pixel + _shadow_width, 220):
        color_n = 0
        for y_cur in range(y_start_cur, y_start_cur + _pixel):
            color_n += img.getpixel((x_cur, y_cur))
        gap_left[x_cur] = color_n
    _maybe = []
    for x_cur in gap_left:
        if gap_left[x_cur] in range(int(y_max_col * 0.85), int(y_max_col * 1.3)):
            _maybe.append(x_cur)
    print(f'找到缺口可能位置{_maybe}')
    # 没找到暂时返回滑块长度加滑块起始位置
    if len(_maybe) == 0:
        return 42 + slider_left_x_index, slider_left_x_index
    elif len(_maybe) == 1:
        return _maybe[0], slider_left_x_index
    # 多个结果，则找相邻（缺口内不会有太多干扰元素）结果间差距在38-43之间的第一个数
    _max_diff = {}
    for i in range(len(_maybe) - 1):
        if _maybe[i + 1] - _maybe[i] in range(38, 43):
            return _maybe[i], slider_left_x_index
        else:
            _max_diff[_maybe[i + 1] - _maybe[i]] = _maybe[i]
    return _max_diff[max(_max_diff)], slider_left_x_index
```

这部分代码有点重复，拼块左描边白点数以及y轴范围已经在`get_start_point`函数中求得了，直接返回使用即可。缺口开始的位置可能是x轴索引48到52（根据拼块的真正起始点而定），一直到`259-42=217`的范围内（代码中我直接写了220），因为拼块的外部阴影造成了多重描边，所以我们还需要剔除掉右描边的阴影部分。大家能看到我定的规则是白点数满足(0.85至1.3)倍的拼块左描边白点数，即视为可能的位置。实际上它们白点数应该是相差极少，甚至可以说是相等的，大家可以通过实验去调优这里的倍数参数，也许还能剔除掉更多的干扰。最后考虑到得到的结果有可能除了真正的起点和终点外，还有其他的一些干扰，所以做了简单的判断，实际上就当前所知的背景图来看，不会出现这种情况。到这里我们已经得到了解决问题的所有钥匙了，接下来就是用钥匙打开新世界大门的时候了。


 #### 3、将拼图移动到缺口处

这一节相对比较简单，拖动操作使用`selenium`的`ActionChains`来实现，上面得到的缺口起始x值和拼块真实的起点值相减，就得到了移动的实际距离，然后直接拖动过去就可以了？确实可以，可以重头来过了。极验会根据整个移动过程的速度判断你是机器还是人，不是说快慢的问题，而是正常的人类行为在移动中速度是会变化的，最真实的情况就是你在接近缺口的时候会减速。根据物理学原理（早就忘了），我写了一个匀加速运动（实际上并不匀）来模拟人类拖动，因为`move_by_offset`会自动将偏移量转成整形类型，所以我还是自己先转好了。
```python
 def btn_slide(browser, x_offset=0, _x_start=6):
    # 开始位置右偏6-13像素
    x_offset = abs(x_offset - _x_start + 1)
    slider = browser.find_element_by_class_name("geetest_slider_button")
    ActionChains(browser).click_and_hold(slider).perform()
    section = x_offset
    left_time = 1
    x_move_list = get_x_move_speed(x_offset, left_time, section)
    print(f'魔鬼的步伐：{x_move_list}')
    print(f'实际应该移动距离:{sum(x_move_list)}')
    for x_move in x_move_list:
        ActionChains(browser).move_by_offset(x_move, yoffset=0).perform()
    ActionChains(browser).release().perform()
    time.sleep(2)
    browser.close()

def get_x_move_speed(distance=0, left_time=0, section=10):
    origin_speed = distance * 2
    acc_speed = origin_speed / left_time / left_time / section
    move_offset = []
    new_speed = origin_speed
    for i in range(0, section):
        new_speed = new_speed - acc_speed
        move_offset.append(round(new_speed / section))
        if sum(move_offset) >= distance or (round(new_speed / section)) == 0:
            break
    if sum(move_offset) < distance:
        move_offset.append(distance - sum(move_offset))
    return move_offset
```

 - 最后的实际效果

<div>
<video id="video" controls="" preload="none" width="100%" poster="https://raw.githubusercontent.com/tomhaoye/geetest-crack-demo/master/pic/poster.jpg">
      <source id="mp4" src="https://github.com/tomhaoye/geetest-crack-demo/blob/master/mp4/20181001_210311.mp4?raw=true" type="video/mp4">
      <p>Your user agent does not support the HTML5 Video element.</p>
</video>
</div>
 
 好了，我的整个实验就到这里就结束了，下面还有一些其他方面的思考。如果有能看到这里的童鞋，既然来了，不如在评论留下一个脚印？

 #### 4、后续：关于干扰块排除的想法

相信大家都能发现，背景图里面，很多时候会在随机的位置出现另外一个或多个"缺口"，它们的颜色相对真正的缺口来说浅一些，但是对于上面方案二的描边法来说，它们确实也是会被描绘出来，虽然我们利用拼块确定了y轴的范围，但是当这些假的"缺口"也在这个y轴范围内的时候，特别是凹凸和方向完全跟真实缺口一样的时候，我们上面的代码就没办法分辨到底哪个才是真正的缺口了。但是解决这个问题的办法也比较简单，我这里想到的思路有两种：
 - 得到的两个位置获取它们原图颜色，相对较深的为真
 - 所有得到的缺口位置都记录下来，先移动到第一个缺口位置，若是不成功，再移动到下一个缺口位置

不管怎样的干扰，总不会喧宾夺主。因为毕竟是给人类辨认的，而机器，只要你制定适当的规则，它们的辨识能力能够达到远超人类的水准。

 #### 5、后续：关于增加定位模拟难度的想法

相信大家看完了实验的整个过程，在这里一定会有很多关于如何设计更加复杂、更有效防止机器模拟行为的验证码的想法，我这里也就这次实验中的滑动验证码发表一下自己的看法。

可以说滑动验证码是国内一种比较新颖的人机校验方式，因为他的操作简单直观，适用于任何年龄层的互联网用户。那实际人机校验的效果如何呢？如果仔细读完上文或者对滑动验证码有研究的童鞋，应该都知道，它实际上是进行了两次校验：识别和行为，我个人认为它能够更好的保护我们的应用以及用户数据，所以现在在国内它的普及度也是相当高。而我这次的实验针对当前版本进行制定规则以实现了机器模拟的目的，但我认为滑动验证码可以通过以下的调整来比较有效增加模拟的难度：

 - 拼块滑动不限于x轴，可在图内任意移动
 - 拼块和缺口的形状改为非固定的多边形或不规矩形状

最最最最后，我想说的是，验证码技术经过多年的发展现在已经相当成熟了，除了日常的数字字母验证码，这次实验的滑动验证码，相信大家也听说过无比牛b的12306验证码，还有google的recaptcha等等。虽然当今的验证码领域已经有如此多的优秀方案，但是我相信没有一个方案能够百分百的抵御机器人，你有世界上最优秀的工程师、科学家，他有这个世界上最有钱的老板啊，把你的工程师和科学家撬过来破解还不是两天的事。现代互联网技术就是在这样的攻防游戏中逐渐的发展壮大的，时代的巨轮滚滚向前，跟不上时代步伐的技术只会被碾压得粉碎。

完。